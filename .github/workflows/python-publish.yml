name: news_crawling

on:
  schedule:
    - cron: "*/10 * * * *" #10분마다 실시
  # '0 23 * * *'  # 매일 8시(한국시간기준) 마다 실행
  # push:
  #   branches: [main]
  # pull_request:
  #   branches: [main]

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Chrome
      uses: browser-actions/setup-chrome@latest
      with:
        chrome-version: 'latest'
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install selenium apscheduler pandas

    - name: Run my script
      run: python3 crawling_new.py

    - name: Configure Git
      run: |
        git config --global user.name "ejjang92"
        git config --global user.email "ej_jang_92@naver.com"

    - name: Commit changes
      run: |
        git add AI.csv
        git commit -m "뉴스데이터 수집" || echo "No changes to commit"
    
    - name: Pull latest changes from main
      run: git pull --rebase origin main

    - name: Push changes
      run: |
        git config --local http.extraHeader "AUTHORIZATION: bearer ${{ secrets.GITHUB_TOKEN }}"
        git push origin main
