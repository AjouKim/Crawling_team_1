name: news_crawling

on:
  schedule:
    - cron: "*/10 * * * *" #10분마다 실시
    #'0 23 * * *'  # 매일 8시(한국시간기준) 마다 실행
  # push :
  #   branches : [ main ]
  # pull_request:
  #   brancges : [ main ]

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

      # 크롬브라우저로 실행
    - name: Set up Chrome
      uses: browser-actions/setup-chrome@latest
      with:
        chrome-version: 'latest'
      # 파이썬 버전 3.11
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.11'

      #실행 시 pip 업그레이드
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install selenium apscheduler pandas

      #크롤링 코드 실행(csv생성까지)
    - name: Run my script
      run: python3 crawling_new.py

      # csv 저장을 위한 부분(수정 필요)
    - name: Configure Git
      run: |
        git config --global user.name "ejjang92"
        git config --global user.email "ej_jang_92@naver.com"
      # 커밋메세지 "뉴스데이터 수집"
    - name: Commit changes
      run: |
        git add AI.csv
        git commit -m "뉴스데이터 수집" || echo "No changes to commit"
      
      # 깃헙에 변경된 작업 있을 시 풀 진행 후 csv파일 푸쉬하기 위한 부분, 레퍼지토리에서 풀 권한 부여해줘야 실행됨
    - name: Pull latest changes from main
      run: git pull --rebase origin main

      # 푸쉬 / githuv_token은 레퍼지토리 토큰 필요
    - name: Push changes
      uses: ad-m/github-push-action@master
      with:
        # 수정필요!!!!!!!!!!!!
        github_token: $  #{{ secrets.NEWSCRAWLER }}
